{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object recognition with keras\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l0MYxW1PyZl1qEA1O/giphy.gif\" align=\"right\" width=\"250\" />\n",
    "\n",
    "Hi again! Let's do some object recognition with keras! We'll make a CNN from the ground up, one that's specialized to **recognize hand drawn doodles**. \"Like pictionary?\" Yes exactly like pictionary!\n",
    "\n",
    "We've seen the basics of how CNN's process visual data in [the previous section](./1.convolutional_neural_networks), we'll compose a net that accepts a picture you draw yourself and guess what's on it! This may not have that much practical use, but it serves as a nice **learning experience** on how neural nets work step by step before you tackle the challenges ahead!\n",
    "\n",
    "We'll use part of google's generously documented ['quick, draw!' dataset](https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/simplified) to train our model.\n",
    "\n",
    "We're using the simplified dataset, but still, there are too many categories to download all of them! That's alright, **pick out two classes** and download them to your machine for use, you'll use those in this notebook for object recognition!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the data\n",
    "\n",
    "Let's take a look at what we dragged out of google's database. For this exercise, I'll be using the **cat** and **dog** classes, but as stated before, you can choose your own classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy -P ../assets/\n",
    "!wget -N https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy -P ../assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "dogs = np.load(\"../assets/dog.npy\")\n",
    "cats = np.load(\"../assets/cat.npy\")\n",
    "\n",
    "plt.bar([0,1], [dogs.shape[0], cats.shape[0]])\n",
    "plt.title('dataset sizes')\n",
    "plt.xticks([0,1], ['dogs', 'cats'])\n",
    "plt.ylabel('number of samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of cats and dogs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google has been so kind to deliver us a **cleaned dataset**, the process of cleaning was as follows:\n",
    "\n",
    "1. Align the drawing to the top-left corner, to have minimum values of 0.\n",
    "2. Uniformly scale the drawing, to have a maximum value of 255.\n",
    "3. Resample all strokes with a 1 pixel spacing.\n",
    "4. Simplify all strokes using the Ramer–Douglas–Peucker algorithm with an epsilon value of 2.0.\n",
    "\n",
    "This would have given them an image like the following.\n",
    "![Cat Drawing (Image)](../assets/cat_drawing.png)\n",
    "\n",
    "To get to an image like we had before, they had to transform this datatype from **vector format** to **raster format**! You might wonder why this step is necessary. Vector format retains all of the original information and is easy to store; no need to store all pixels, only 2 points for every line. BUT...convolutional neural networks want all those intermediate pixels as well! We want a **grid of pixels** to feed to our network so we can **convolve and pool**.\n",
    "\n",
    "Fortunately, the `.npy` file format contains the pixel values of the cats and dogs in binary, so we do not need to clean them more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raster(image):\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "dog_sample = dogs[0].reshape(28,28)\n",
    "plot_raster(dog_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a cute little doggy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sample = cats[1].reshape(28,28)\n",
    "plot_raster(cat_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go, the same kitty as above but in a raster format! When this is fed to a **CNN**, the input layer can accept this input as an **image** comprising as a 28 by 28 grid of **pixels** that are either activated or not. This way, our neural network can also **convolve** the image and **pool** the pixels. This would be a bit hard to do with the original **vector format**.\n",
    "\n",
    "We have more than enough samples to work with, so let's take a nicely **balanced subset** of our data so we don't get bias in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40000 samples is a nice balance to have enough data to have a nice\n",
    "# accuracy, without training for too long\n",
    "max_samples = 40000\n",
    "preprocessed_cats = cats[:max_samples].reshape(-1,28,28)\n",
    "preprocessed_dogs = dogs[:max_samples].reshape(-1,28,28)\n",
    "\n",
    "# Normalizing\n",
    "preprocessed_cats = preprocessed_cats/255\n",
    "preprocessed_dogs = preprocessed_dogs/255\n",
    "\n",
    "plt.bar([0,1], [preprocessed_dogs.shape[0], preprocessed_cats.shape[0]])\n",
    "plt.title('dataset sizes')\n",
    "plt.xticks([0,1], ['dogs', 'cats'])\n",
    "plt.ylabel('number of samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the labels as well! Our neural network won't like **string values like 'cat' and 'dog**, but will want numeric representations instead. let's use **0 for cats, and 1 for dogs**. When you handle classifiers with more than two possible classes, don't forget the one-hot encoding format! Keras has the handy [to_categorical](https://keras.io/api/utils/python_utils/#to_categorical-function) function for this. For this example though, it is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels = np.zeros((max_samples, 1))\n",
    "dog_labels = np.ones((max_samples, 1))\n",
    "\n",
    "labels = np.concatenate([cat_labels, dog_labels])\n",
    "drawings = np.concatenate([preprocessed_cats, preprocessed_dogs])\n",
    "\n",
    "# tensorflow wants a 4D tensor with (n_images, width, height, colour_depth)\n",
    "print(\"Drawings shape before : \", drawings.shape)\n",
    "drawings = np.expand_dims(drawings, axis=3)\n",
    "print(\"Drawings shape after : \", drawings.shape)\n",
    "print(\"Label shape : \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final step: **separating our dataset into a train/val/test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_drawings, test_drawings, train_val_labels, test_labels = train_test_split(\n",
    "    drawings, \n",
    "    labels,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_drawings, val_drawings, train_labels, val_labels = train_test_split(\n",
    "    train_val_drawings, \n",
    "    train_val_labels,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"train_drawings shape : \", train_drawings.shape)\n",
    "print(\"val_drawings shape : \", val_drawings.shape)\n",
    "print(\"test_drawings shape : \", test_drawings.shape)\n",
    "\n",
    "print(\"train_labels shape : \", train_labels.shape)\n",
    "print(\"val_labels shape : \", val_labels.shape)\n",
    "print(\"test_labels shape : \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing and evaluating the model\n",
    "\n",
    "### Composing the most simple form\n",
    "\n",
    "You've seen in [convolutional neural networks](./1.convolutional_neural_networks.ipynb) the different parts of a typical CNN and how they relate to each other. **keras** makes it easy for us to quickly compose a neural network.\n",
    "\n",
    "Let's start with the **simplist of convolutional neural networks**: A **convolutional layer**, followed by **a flattening layer** and **a regular 1D dense layer**. Why is this the most simple form, you ask? Well, our neural network cannot consist of only a conv layer, since these **accept and output a 2D data structure**. If the output and input layer are the same, the outputs is a very hard to interpret **2D layer**. How does this relate to our defined classes; The **cat class** and **dog class**? It doesn't, it needs to be flattened into a binary output, since we have two classes in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fix some basic parameters below. You can use these values directly, they have been tested by us to be good enough for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# pixel width and height of our images\n",
    "input_size = 28\n",
    "\n",
    "# number of filters in the convnet layer\n",
    "filters = 64\n",
    "\n",
    "# conv net parameters\n",
    "strides = (2, 2)\n",
    "pool_size = (2,2)\n",
    "kernel_size = (5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know how `tf.keras` works now. You'll implement the model described above using the `Sequential` API.\n",
    "You also get a model summary below to show the output shapes.\n",
    "In this first example, we'll only modify the default `kernel_size`, `activation` (use ReLU) and `input_shape` of the convolutional layer. We also specify the number of convolutional filters, given by the `filters` variable. The other parameters are left at their default values.\n",
    "\n",
    "\n",
    "Store the model in a `model` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first version of the model\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary should look similar to the following:\n",
    "![Keras model 1 summary](../assets/first_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In Tensorflow, we have a bit more control over what we can do with the loss function than in PyTorch.\n",
    "Indeed, in this case, we have the choice between 4 different losses, which depend on:\n",
    "- whether we one-hot encode our labels\n",
    "- whether we use a Softmax activation in the last layer\n",
    "\n",
    "The next will show the 4 solutions.\n",
    "\n",
    "\n",
    "\n",
    "- We can put a `SoftMax()` on the last layer, **NOT** one-hot encode the labels and use [`tf.keras.losses.SparseCategoricalCrossentropy()`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)\n",
    "\n",
    "\n",
    "- We can **NOT** put a `Softmax()` on the last layer, **NOT** one-hot encode the labels and use [`tf.nn.sparse_softmax_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits). This is equivalent to the above loss with the argument `from_logits` set to `True` : `SparseCategoricalCrossentropy(from_logits=True)` because the default is `False`.\n",
    "\n",
    "\n",
    "- We can put a `SoftMax()` on the last layer, and one-hot encode labels and use [`tf.keras.losses.CategoricalCrossentropy(from_logits=False)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy)\n",
    "\n",
    "\n",
    "- We can **NOT** put a `SoftMax()` on the last layer, and one-hot encode labels and use `tf.keras.losses.CategoricalCrossentropy(from_logits=True)`\n",
    "\n",
    "\n",
    "**Logits** is a confusing term and often abused in the deep learning community, as it can mean a lot of things. Here, logits are meant to describe the input to the Softmax layer, or generally any output from a neuron that has not been passed through an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it is on you to choose the correct loss function** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Adam optimizer, one of the above mentioned loss functions and the accuracy as a metric.\n",
    "\n",
    "Then fit the model for `6` epochs, while not forgetting to use the validation set.\n",
    "\n",
    "Store the result of the fit in `history` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate out model by comparing our **validation accuracy vs the training accuracy** with the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history : tensorflow.python.keras.callbacks.History):\n",
    "    \"\"\" This helper function takes the tensorflow.python.keras.callbacks.History\n",
    "    that is output from your `fit` method to plot the loss and accuracy of\n",
    "    the training and validation set.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "    axs[0].plot(history.history['accuracy'], label='training set')\n",
    "    axs[0].plot(history.history['val_accuracy'], label = 'validation set')\n",
    "    axs[0].set(xlabel = 'Epoch', ylabel='Accuracy', ylim=[0, 1])\n",
    "\n",
    "    axs[1].plot(history.history['loss'], label='training set')\n",
    "    axs[1].plot(history.history['val_loss'], label = 'validation set')\n",
    "    axs[1].set(xlabel = 'Epoch', ylabel='Loss', ylim=[0, 10])\n",
    "    \n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[1].legend(loc='lower right')\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 85% accuracy...Alright, not bad! But we can do better!\n",
    "Notice that the validation loss does not increase anymore.\n",
    "\n",
    "**Having only a single convolutional layer is maybe not enough to perfectly tell the difference between cat and dog doodles**. Why? Because right now, the neural network can just about make out if an image contains lines or not, which they both obviously do! Look at the image below:\n",
    "\n",
    "![convnet_progression](./../assets/convnet_progression.png)\n",
    "\n",
    "Each of these stages (from bottom to top) represent a convnet layer, and what it outputs. The bottom layer has 24 filters and seems to be able to make out the lines composing the image, but it's not connecting the dots so to speak. The features extracted by the first convnet layer are given to the next, and this layer connect these features into higher order features, and so on.\n",
    "\n",
    "So we are still stuck in the first stage, we need to **extend our model**.\n",
    "\n",
    "### Composing an extended model\n",
    "\n",
    "Let's add another convnet layer! But in between these, we'll add a **pooling layer**. What's the purpose of this layer you ask? It has multiple purposes as seen in the last subchapter, but the main purpose here is **dimensionality reduction** to make it a bit easier on our model. Training the last model was already pretty challenging, and we don't need quantity, we need quality. Pooling (supposedly) redundant spatial information together is ofter done in CNN's for this reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you know what to do. We will be modifying the same parameters as before, and you have the summary below again for the details. \n",
    "\n",
    "For the **pooling layer**, we will specify our own `strides` and `pool_size` parameters. Also specify `padding='same'`. \n",
    "\n",
    "Make sure to change those so that your summary matches the one provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the second version of the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary should look similar to the following:\n",
    "![Keras model 2 summary](../assets/second_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, you can run the model for a few more epochs, to give it time to get better.\n",
    "Use around `10` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 88% ! YEAH, we added 3% to our accuracy! Right now, our model is still lacking something vital: **regular dense layers**! \n",
    "\n",
    "Adding these layers is a nice way to finally put the extracted features together and make nonlinear connections between them! Like \"aha, sharp ear features + a triangle nose means a cat, probably >.>!\".\n",
    "\n",
    "We will also add some **Dropout** layers to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further extending the model with dense layers\n",
    "Last model, like said above, we're adding `Dense` layers at the end and `Dropout` layers between the convolutional layers and the pooling layers. Remember that we are still modifying the same parameters for the **convolutional layer** and for the **pooling layer** as the two other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final version of the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary should look similar to the following:\n",
    "![Keras model 3 summary](../assets/third_summary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model (10 epochs too)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 89% accuracy ?! Now we're cookin'! This is starting to look good! As you already know, there are a few ways to combat overfitting in a model. One of them is to **decrease the DOF's of the model**, another neat method often used in CNN's is **adding dropout layers**.\n",
    "\n",
    "\"Dropout layers?! I'm no quitter!\". Don't worry, these are conceptually easy to understand: These **deactivate a fraction of the neurons of the preceeding layers at random** to encourage robustness on the overall layer. Deactivating them at random avoids the reliance on specific nodes and disallows the network to overly fit to the training set.\n",
    "\n",
    "This method isn't perfect, and it needs a decently sized network, which may not be the case here, so it might not be necessary here.\n",
    "\n",
    "Read more about dropout [here](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You created your first convolutional neural networks from scratch, and were able to get a pretty good accuracy with a small network.\n",
    "\n",
    "![Well done (GIF)](https://media.giphy.com/media/d31w24psGYeekCZy/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "The following is a piece of code designed to take inputs from the user to generate a drawing from. **This code is very case specific and there is no need for you to learn the tkinter library for computer vision**. So just enjoy the result and continue on to the [next notebook](./4.object_recognition_with_pytorch.ipynb) after you're done playing around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDrawer:\n",
    "    \n",
    "    def __init__(self, canvas, input_size, output_size = 64, brush_size = 3, line_colour = \"#476042\"):\n",
    "        self.canvas = canvas\n",
    "        self.drawing = np.zeros((output_size, output_size))\n",
    "        self.mouse_is_clicked = False\n",
    "        self.brush_size = brush_size\n",
    "        self.line_colour = line_colour\n",
    "        self.scale_factor = float(output_size)/float(input_size)\n",
    "\n",
    "    def on_motion(self, event):\n",
    "        if not self.mouse_is_clicked:\n",
    "            return\n",
    "        x1, y1 = (event.x - self.brush_size), (event.y - self.brush_size)\n",
    "        x2, y2 = (event.x + self.brush_size), (event.y + self.brush_size)\n",
    "        self.canvas.create_oval(x1, y1, x2, y2, fill=self.line_colour)\n",
    "        self.drawing[round(event.y*self.scale_factor), round(event.x*self.scale_factor)] = 255\n",
    "\n",
    "    def on_down_press(self, event):\n",
    "        self.mouse_is_clicked = True\n",
    "\n",
    "    def on_release(self, event):\n",
    "        self.mouse_is_clicked = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "# run this cell to get a drawing interface! Just close it once you are done\n",
    "gui = tk.Tk()\n",
    "canvas_size = 512\n",
    "canvas = tk.Canvas(gui, width=canvas_size, height=canvas_size)\n",
    "line_drawer = LineDrawer(canvas, canvas_size, 28)\n",
    "canvas.bind('<Motion>', line_drawer.on_motion)\n",
    "canvas.bind('<Button-1>', line_drawer.on_down_press)\n",
    "canvas.bind('<ButtonRelease-1>', line_drawer.on_release)\n",
    "canvas.pack()\n",
    "gui.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_raster(line_drawer.drawing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_input = line_drawer.drawing.reshape(1, 28, 28, 1)\n",
    "prediction = model.predict(drawing_input)\n",
    "print(f\"Predictions : {prediction[0][0] * 100 :.2f}% cat, {prediction[0][1] * 100 :.2f}% dog \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to get a drawing interface!\n",
    "gui = tk.Tk()\n",
    "canvas_size = 512\n",
    "canvas = tk.Canvas(gui, width=canvas_size, height=canvas_size)\n",
    "second_line_drawer = LineDrawer(canvas, canvas_size, 28)\n",
    "canvas.bind('<Motion>', second_line_drawer.on_motion)\n",
    "canvas.bind('<Button-1>', second_line_drawer.on_down_press)\n",
    "canvas.bind('<ButtonRelease-1>', second_line_drawer.on_release)\n",
    "canvas.pack()\n",
    "gui.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_raster(second_line_drawer.drawing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_input = second_line_drawer.drawing.reshape(1, 28, 28, 1)\n",
    "prediction = model.predict(drawing_input)\n",
    "print(f\"Predictions : {prediction[0][0] * 100 :.2f}% cat, {prediction[0][1] * 100 :.2f}% dog \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
