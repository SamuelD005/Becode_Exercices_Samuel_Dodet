{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object recognition with opencv\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/26tPkdXUVIRqaWWu4/giphy.gif\" align=\"right\" width=\"250\"/>\n",
    "\n",
    "Welcome, welcome, to **accessible computer vision** with [OpenCV](https://docs.opencv.org/4.5.1/)! **OpenCV** is short for **'open computer vision'**, so you can imagine what this **library** is all about!\n",
    "\n",
    "OpenCV is a collection of computer vision methods, including methods described in the [README](../README.md) file of this chapter. The focus of this chapter is **object recognition**; the act of **classifying images**. This is a **supervised learning method**.\n",
    "\n",
    "OpenCV makes it easy for us to quickly get something up and running, without too much hassle. There are many **available CNN network architectures available**, some of which are **already fully trained**. They're out-of-the-box image classifiers! So before we delve deeper into how they work, it may be useful for you to be able to use OpenCV's available tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detecting regions of interest\n",
    "\n",
    "Alright, for many object recognition tasks, you would commonly want to define **regions of interest**, also known as **ROI**. This is an important step of object recognition, since you would like to provide the actual CNN an image with **only the object of interest**, instead of an image containing many objects. An image like that would confuse the CNN and result in poor performance.\n",
    "\n",
    "There's multiple ways to go about this, OpenCV even has its own object detection classifier based on [Haar feature-based cascade classifiers](https://docs.opencv.org/master/db/d28/tutorial_cascade_classifier.html), but it is highly dependent on what you want to detect.\n",
    "\n",
    "Even though the built-in method of OpenCV works well, there's a handy feature of OpenCV that will come in handy for years to come; **importing existing award-winning NN models**. For this exercise, we'll want to **detect faces**, since we want to recognize something about this face. So, we'll import the **[ResNet](https://towardsdatascience.com/introduction-to-resnets-c0a830a288a4)** model architecture **trained to detect the presence of a face in a particular area of an image**. No need to concern yourself too much with what's going on under the hood just yet, we'll take a look at that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# Load our serialized face detector model from disk\n",
    "layer_info_path = \"../models/deploy_res_net.prototxt\"\n",
    "weight_info_path = \"../models/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "face_net = cv.dnn.readNet(layer_info_path, weight_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"**Prototxt**? **Caffemodel**? What the hell are these alien sounding extensions?\"\n",
    "\n",
    "Cool your jets, hotshot. These are simply some **[widely used NN model formats](https://caffe.berkeleyvision.org/)**. The `.prototxt` file contains info on the model structure; the number of layers, nodes, activation functions,... while the `.caffemodel` file contains the weights to all of the trainable parameters of the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(\"../assets/guyfieri.jpg\")\n",
    "(height, width) = image.shape[:2]\n",
    "\n",
    "# Construct input blob for the NN\n",
    "\n",
    "def getBlobFromImage(image, scale_factor, output_size, mean_substraction) -> np.ndarray:\n",
    "    \"\"\"Get blob of size `output_size` from the `image` at a given `scale_factor`,\n",
    "    and substract the `mean_substraction` from every pixel of the image.\"\"\"\n",
    "    return cv.dnn.blobFromImage(image, scale_factor, output_size, mean_substraction)\n",
    "\n",
    "def getGeneralBlob(image) -> np.ndarray:\n",
    "    \"\"\"Get a blob from the whole image\"\"\"\n",
    "    scale_factor = 1.0\n",
    "    output_size = (300, 300)\n",
    "    general_mean_subtraction = (104.0, 177.0, 123.0)\n",
    "    return getBlobFromImage(image, scale_factor, output_size, general_mean_subtraction)\n",
    "\n",
    "\n",
    "blob = getGeneralBlob(image)\n",
    "blob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can probably guess what the scale factor and output size do, but that **mean subtraction** and **blob** may raise some eyebrows. In short, mean subtraction is an image preprocessing technique often used to **combat illumination differences** between images and **increase contrast**. \n",
    "\n",
    "Informally, a **blob** is just a (potentially collection) of image(s) with the same spatial dimensions (i.e., width and height), same depth (number of channels), that have all be preprocessed in the same manner.\n",
    "\n",
    "So here, we have a collection of 1 image, with 3 channels (BGR) and of shape 300x300, like requested.\n",
    "\n",
    "More on that right [here](https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/). \n",
    "\n",
    "Don't lose yourself in that article though, the important thing to know is that we know have a processed **uniform blob** that our model accepts without any hassle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetections(blob) -> np.ndarray:\n",
    "    \"\"\"Pass the `blob` forward through the face detection model, and return\n",
    "    the detected faces, with confidence values and coordinates\"\"\"\n",
    "\n",
    "    face_net.setInput(blob)\n",
    "    detections = face_net.forward()\n",
    "    return detections.squeeze() # We remove single dimensions (1,1,200,7) --> (200,7)\n",
    "\n",
    "detections = getDetections(blob)\n",
    "detections.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, that was pretty easy, wasn't it? Now we have a set (200 to be exact) of **detection blobs** with associated **confidence values** (column 2) for each of the defined areas of the model. Our resnet model has **many output nodes**, each of which corresponds to a different image area, with additional nodes that correspond to **width** and **height** (columns 3 to 7) of the detected face.\n",
    "\n",
    "So, the key take-away here is that the **output blob dimensions** are dependant on the **model architecture**. This is pretty important information if you want to properly index these outputs for further processing, like we do down below. The resnet model has a pretty complex output structure, no need to concern yourselves too much with this, since they're properly indexed here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create some helper methods to properly section our code so that it is easier to understand and to reduce code duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum confidence needed to detect face\n",
    "minimum_confidence = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBox:\n",
    "    \"\"\"Create a colored bounding box to draw over the image.\"\"\"\n",
    "    def __init__(self, start_x, start_y, end_x, end_y):\n",
    "        self.start_x = start_x\n",
    "        self.start_y = start_y\n",
    "        self.end_x = end_x\n",
    "        self.end_y = end_y\n",
    "        \n",
    "        self.color = (0,0,255) # RED (BGR)\n",
    "        self.thickness = 2\n",
    "\n",
    "    def drawOn(self, image) -> None:\n",
    "        \"\"\"Draw a rectangular bounding box on the `image` using `cv.rectangle`\n",
    "        \n",
    "        Side effect: the `image` object is modified.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        image : np.ndarray\n",
    "            Image to draw the bounding box on.\n",
    "        \"\"\"\n",
    "        cv.rectangle(\n",
    "            image, \n",
    "            (self.start_x, self.start_y), \n",
    "            (self.end_x, self.end_y), \n",
    "            self.color, \n",
    "            self.thickness\n",
    "        )\n",
    "\n",
    "def get_bounding_box(detection, width, height) -> BoundingBox:\n",
    "    \"\"\" Computes the (x, y)-coordinates of the bounding box.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        detection : np.ndarray\n",
    "            Detected image of a face.\n",
    "        width : int\n",
    "            Height of the whole image.\n",
    "        height : int\n",
    "            Height of the whole image.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        BoundingBox\n",
    "            Bounding box surrounding the face.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Coordinates as a fraction of the total width/height\n",
    "    coordinate_fractions = detection[3:7]\n",
    "    box = coordinate_fractions * np.array([width, height, width, height])\n",
    "    (start_x, start_y, end_x, end_y) = box.astype(\"int\")\n",
    "    return BoundingBox(start_x, start_y, end_x, end_y)\n",
    "\n",
    "\n",
    "def plot_image(image) -> None:\n",
    "    \"\"\"Plot the image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        image : np.ndarray\n",
    "            Image to plot.\n",
    "    \"\"\"\n",
    "    # Convert image from bgr to rgb format\n",
    "    rgb_image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the output image\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.axis('off')\n",
    "    plt.title('flavortown')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the actual face detection using our defined methods.\n",
    "\n",
    "Here, we loop through all of the potential detected faces, only selecting\n",
    "those that the algorithm is most confident about and drawing a bounding box around it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detection in detections:\n",
    "    # Extract the confidence (i.e., probability) associated with the prediction\n",
    "    confidence = detection[2]\n",
    "    \n",
    "    if confidence > minimum_confidence:\n",
    "        bounding_box = get_bounding_box(detection, width, height)\n",
    "        bounding_box.drawOn(image)\n",
    "\n",
    "plot_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! We're feeling the heat, but we're not all the way in Flavortown just yet! We've detected a **ROI**; Guy Fieri's face, but we can do further analysis on him, now that we **isolated** his most defining feature (apart from his timeless frosted tips).\n",
    "\n",
    "## 2. Classifying regions of interest\n",
    "\n",
    "Let's see **how old** Guy Fieri was at the time of this iconic picture. Or at least, [this](https://talhassner.github.io/home/publication/2015_CVPR) model's estimation of Fieri's age. **Age** is a pretty subjective metric to evaluate, so the results may be a bit off, though we can **relax** the problem a bit by letting the classifier classify people in different **time-ranges** (e.g. 25-30).\n",
    "\n",
    "For this, we need to load said model just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our serialized age classifier model from disk\n",
    "layer_info_path = \"../models/deploy_age_net.prototxt\"\n",
    "weight_info_path = \"../models/age_net.caffemodel\"\n",
    "age_net = cv.dnn.readNet(layer_info_path, weight_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define the **age buckets** (these are dictated by the model paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of age buckets our age detector will predict\n",
    "age_buckets = [\"(0-2)\", \"(4-6)\", \"(8-12)\", \"(15-20)\", \"(25-32)\", \"(38-43)\", \"(48-53)\", \"(60-100)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll add some lines of code to also detect age right after defining the ROI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_blob(image) -> np.ndarray:\n",
    "    \"\"\" Create blobs from an`image` of a face with the aim of \n",
    "    predicting an age.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Image of a face.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    np.ndarray\n",
    "        Blob of the face.\n",
    "    \"\"\"\n",
    "    scale_factor = 1.0\n",
    "    # Mean subtraction values for faces (predetermined)\n",
    "    face_mean_subtraction = (78.0, 87.0, 115.0)\n",
    "    # Output size of the cropped face\n",
    "    face_output_size = (227, 227)\n",
    "    return getBlobFromImage(image, scale_factor, face_output_size, face_mean_subtraction)\n",
    "\n",
    "def extract_face_ROI(image, bounding_box : BoundingBox) -> np.ndarray:\n",
    "    \"\"\" Extract the region of interest (ROI) (the face) from the image and construct a\n",
    "    blob from it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Image from which to extract a face.\n",
    "\n",
    "    bounding_box : BoundingBox\n",
    "        Bounding box delimiting a face.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    np.ndarray\n",
    "        Blob of the face.\n",
    "    \"\"\"\n",
    "    face = image[bounding_box.start_y:bounding_box.end_y, \n",
    "                 bounding_box.start_x:bounding_box.end_x]\n",
    "    return get_face_blob(face)\n",
    "\n",
    "\n",
    "def predict_age(age_net, face_blob, age_buckets) -> Tuple[str, float]:\n",
    "    \"\"\"Make predictions on the age from the `face_blob` using `age_net`\n",
    "    and find the age bucket with the largest corresponding confidence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    age_net : cv.dnn\n",
    "        Neural network to predict the age.\n",
    "\n",
    "    face_blob : np.ndarray\n",
    "        Blob of the face.\n",
    "\n",
    "    age_buckets : list[str]\n",
    "        List of age ranges\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    age : str\n",
    "        Predicted age range from `age_buckets`\n",
    "    age_confidence : float\n",
    "        Confidence in the predicted age\n",
    "    \"\"\"\n",
    "    age_net.setInput(face_blob)\n",
    "    predictions = age_net.forward()\n",
    "    max_conf_index = predictions[0].argmax()\n",
    "    age = age_buckets[max_conf_index]\n",
    "    age_confidence = predictions[0][max_conf_index]\n",
    "    \n",
    "    return age, age_confidence\n",
    "\n",
    "def draw_age_range(image, age, age_confidence, bounding_box : BoundingBox) -> None:\n",
    "    \"\"\"Make predictions on the age from the `face_blob` using `age_net`\n",
    "    and find the age bucket with the largest corresponding confidence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Image to draw the age range on.\n",
    "    age : str\n",
    "        Predicted age range.\n",
    "    age_confidence : float\n",
    "        Confidence in the predicted age range.\n",
    "    bounding_box : BoundingBox\n",
    "        Bounding box delimiting a face.\n",
    "    \"\"\"\n",
    "    # Draw the age range above the bounding box\n",
    "\n",
    "    start_y = bounding_box.start_y\n",
    "    y = start_y - 10 if start_y - 10 > 10 else start_y + 10\n",
    "    text = \"{}: {:.2f}%\".format(age, age_confidence * 100)\n",
    "    cv.putText(image, \n",
    "               text,\n",
    "               (bounding_box.start_x, y), \n",
    "               cv.FONT_HERSHEY_SIMPLEX, \n",
    "               1, \n",
    "               (0, 0, 255), # RED \n",
    "               2) #Line thickness\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the detections\n",
    "for detection in detections:\n",
    "    # Extract the confidence (i.e., probability) associated with the prediction\n",
    "    confidence = detection[2]\n",
    "    \n",
    "    # Filter out weak detections by ensuring the confidence is greater than the minimum confidence\n",
    "    if confidence > minimum_confidence:\n",
    "        bounding_box = get_bounding_box(detection, width, height)\n",
    "        bounding_box.drawOn(image)\n",
    "\n",
    "        face_blob = extract_face_ROI(image, bounding_box)\n",
    "        \n",
    "        age, age_confidence = predict_age(age_net, face_blob, age_buckets)\n",
    "        draw_age_range(image, age, age_confidence, bounding_box)\n",
    "\n",
    "\n",
    "plot_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we've got a result! **38-43** is the classifiers guess. That's close, but no cigar, considering Guy's age at the time of this picture was **just over 50**. Maybe those frosted tips keep him looking eternally youthful.\n",
    "\n",
    "Or there may be other factors at play here. Notice how the **confidence** is also very low, meaning the classifier is having trouble tracking defining age indicators on Fieri's face. **discuss this** with colleagues, what could possible reasons for the model's poor performance with Fieri pics?\n",
    "\n",
    "## 3. Interactive demo\n",
    "\n",
    "Lastly, let's see how old **you** all are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_age(frame, face_net, min_conf=0.5):    \n",
    "    # Initialize our results list\n",
    "    results = []\n",
    "    \n",
    "    # Grab the dimensions of the frame and then construct a blob from it\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = getGeneralBlob(frame)\n",
    "    \n",
    "    # Pass the blob through the network and obtain the face detections\n",
    "    detections = getDetections(blob)\n",
    "    \n",
    "    # loop over the detections\n",
    "    for detection in detections:\n",
    "        # Extract the confidence (i.e., probability) associated with the prediction\n",
    "        confidence = detection[2]\n",
    "        \n",
    "        # Filter out weak detections by ensuring the confidence is greater than the minimum confidence\n",
    "        if confidence > min_conf:\n",
    "            bounding_box = get_bounding_box(detection, w, h)\n",
    "            face_blob = extract_face_ROI(frame, bounding_box)\n",
    "            age, age_confidence = predict_age(age_net, face_blob, age_buckets)\n",
    "\n",
    "            # Construct a dictionary consisting of both the face bounding box location \n",
    "            # along with the age prediction, then update our results list\n",
    "            result = {\n",
    "                \"loc\": bounding_box,\n",
    "                \"age\": (age, age_confidence)\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "    # Return our results to the calling function\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Run this cell to start the live demo'''\n",
    "\n",
    "from imutils.video import VideoStream\n",
    "import imutils\n",
    "\n",
    "# Initialize the video stream\n",
    "vs = VideoStream(src=0).start()\n",
    "\n",
    "# Loop over the frames from the video stream\n",
    "try:\n",
    "    while True:\n",
    "        # Grab the frame from the threaded video stream and resize it to have a maximum width of 400 pixels\n",
    "        frame = vs.read()\n",
    "        frame = imutils.resize(frame, width=400)\n",
    "\n",
    "        # Detect faces in the frame, and for each face in the frame, predict the age\n",
    "        results = classify_age(frame, age_net, min_conf=0.8)\n",
    "\n",
    "        # Loop over the results\n",
    "        for r in results:\n",
    "            # Draw the bounding box(es) of the face(s) along with the associated predicted age\n",
    "            age, age_confidence = r[\"age\"]\n",
    "            bounding_box= r[\"loc\"]\n",
    "            draw_age_range(frame, age, age_confidence, bounding_box)\n",
    "            bounding_box.drawOn(frame)\n",
    "\n",
    "        # Show the output frame\n",
    "        cv.imshow(\"Frame\", frame)\n",
    "        key = cv.waitKey(1) & 0xFF\n",
    "        # If the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "except:\n",
    "    raise\n",
    "finally:\n",
    "    # Do a bit of cleanup\n",
    "    cv.destroyAllWindows()\n",
    "    vs.stop()\n",
    "    vs.stream.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "Alright, now you've had a taste on how to quickly **implementing existing models** using **OpenCV**. If the performance of the classifier isn't to your liking, then **just import another**! That's the neat thing about pre-trained networks, you can use what's already out there.\n",
    "\n",
    "Let's get to looking a bit **under the hood** on how CNN's work in [the next notebook](./5.object_recognition_from_scratch_with_keras.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
