{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the perceptron theory\n",
    "\n",
    "Now, we are diving into the heart of it! Programming our first neural network using `pytorch`.\n",
    "\n",
    "Note, run all the cells every time to prevent unwanted errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "filename = \"./data/iris.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "df = df.sample(frac=1).reset_index(drop=True) # Shuffle dataframe\n",
    "df.head()\n",
    "print(df.shape)\n",
    "print(df[\"species\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a test/train split\n",
    "train_test_split_fraction = 0.80\n",
    "split_index = int(df.shape[0] * train_test_split_fraction)\n",
    "df_train = df[:split_index]\n",
    "df_test = df[split_index:]\n",
    "\n",
    "target = pd.get_dummies(df['species']).values # One hot encode\n",
    "target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features and the target\n",
    "X_train = df_train.drop('species', axis = 1).values\n",
    "X_test = df_test.drop('species', axis = 1).values\n",
    "\n",
    "y_train = target[:split_index]\n",
    "y_test = target[split_index:]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform `X_train`, `y_train` and their test counterparts into pytorch tensors using `torch.tensor()`. Make sure to convert them to float32 with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = #TO COMPLETE \n",
    "y_train = #TO COMPLETE \n",
    "X_test = #TO COMPLETE \n",
    "y_test = #TO COMPLETE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected our train and test sets, we have to create the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_hidden_neurons = 10\n",
    "nb_classes = len(pd.unique(df['species']))\n",
    "nb_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.Linear()` method defines a layer of neurons in the neural network.\n",
    "We want the network to have 10 neurons in the middle layers and 3 in the last layer (one for every class).\n",
    "\n",
    "Notice how we define the layers in `__init__()`, and we connect them in `forward()` .\n",
    "\n",
    "In the code below in the `__init__()` function,  complete the code for `self.layer_1`, `self.layer_2` , and `self.layer_3` using the `nn.Linear()` class to add the desired layers. *Hint: Make use of `nb_features` and `nb_hidden_neurons`. You can also refer to the architecture of the network shown in the image a few cells below*.\n",
    "\n",
    "In the `forward()` function, which is going to handle the forward pass of the neural network learning process, we will use the ReLU activation function for the first layers and for the last layer, we are going to use the Softmax activation function.\n",
    "We want you to assign the ReLU activation function to the variable `activation_function`. More information [here](https://pytorch.org/docs/stable/nn.html).\n",
    "Lastly, we want you to write the code to combine the layers to the input `x` and the output. *Hint: You can refer yourself to how the last layer code is written*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_features):\n",
    "        \"\"\"Here we define the layers\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_1 =  #TO COMPLETE\n",
    "        self.layer_2 =  #TO COMPLETE\n",
    "        self.layer_3 =  #TO COMPLETE\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"Here we combine the layers\n",
    "        \"\"\"\n",
    "        \n",
    "        activation_function = #TO COMPLETE \n",
    "        last_layer_activation = nn.Softmax(dim=1)\n",
    "        \n",
    "        output_first_layer = #TO COMPLETE \n",
    "        output_second_layer = #TO COMPLETE\n",
    "        prediction = last_layer_activation(self.layer_3(output_second_layer))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = Network(nb_features=X_train.shape[1])\n",
    "my_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything right, this should be the output of the previous call\n",
    "\n",
    "```\n",
    "Network(\n",
    "  (layer_1): Linear(in_features=4, out_features=10, bias=True)\n",
    "  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n",
    "  (layer_3): Linear(in_features=10, out_features=3, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have done the architecture of the model. Right now, it does nothing. We have to make it learn.\n",
    "\n",
    "First, we'll need to define the `criterion` by which the cost if calculated.\n",
    "There are many possible losses to choose from (see #Additional reading material), but here we're going to choose the `MSELoss`.\n",
    "The piece of code that is going to compute the gradient and change the weights is called an `optimizer`.\n",
    "Again, there are many to choose from (see #Additional reading material), but here we are going to use the `Adam` optimizer with a `learning_rate` of `0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your criterion, your learning rate and your optimizer.\n",
    "criterion = #TO COMPLETE \n",
    "learning_rate = #TO COMPLETE \n",
    "optimizer = #TO COMPLETE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start training the neural network. Below, the `training()` function handles all the necessary stuff for an efficient training process.\n",
    "The loops, the input and output getting, and the loss plotting are already taken care of.\n",
    "You just have to do the most important part of the training process, which is listed as a comment.\n",
    "\n",
    "You can run the function with the next cell, which will also plot a graph of the loss over the whole training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(batch_size : int, nb_steps_loss_sum : int):\n",
    "    \"\"\" Train the neural network, feeding it `batch_size` at a time\n",
    "    and saving statistics every `nb_steps_loss_sum` steps.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    - batch_size [int] : the number of input samples at each training step (called a batch)\n",
    "    - nb_steps_loss_sum [int] : the number of batches before saving the loss for plotting\n",
    "    \n",
    "    Returns:\n",
    "    - loss_list : [List[double]] : value of the loss every `nb_steps_loss_sum` steps\n",
    "    \"\"\"\n",
    "\n",
    "    loss_list = []\n",
    "    running_loss = 0\n",
    "    batch_nb = 0\n",
    "\n",
    "    for epoch in range(0,10): # Number of times to iterate through the complete dataset\n",
    "        for idx in range(0, X_train.shape[0], batch_size):\n",
    "            \n",
    "            # Get input and output\n",
    "            input_batch = X_train[idx:idx + batch_size]\n",
    "            target = y_train[idx:idx + batch_size]\n",
    "            \n",
    "            # TO COMPLETE:\n",
    "            # - zero gradient buffers\n",
    "            # - compute the forward pass\n",
    "            # - compute the loss\n",
    "            # - backpropagate\n",
    "            # - do a step\n",
    "          \n",
    "            \n",
    "            # Save the loss every `running_loss_steps` batches\n",
    "            running_loss += loss.item()\n",
    "            save_loss_condition = batch_nb % nb_steps_loss_sum == (nb_steps_loss_sum - 1)\n",
    "            if save_loss_condition:    \n",
    "                loss_list.append(running_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "            batch_nb+= 1\n",
    "        \n",
    "    return loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_steps_loss_sum = 10\n",
    "loss = training(batch_size=1, nb_steps_loss_sum=nb_steps_loss_sum)\n",
    "\n",
    "# Plotting the loss over training\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(loss)), loss)\n",
    "plt.xlabel(f\"Batches/{nb_steps_loss_sum}\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went to plan, you should have a decreasing loss over time. Something similar to this:\n",
    "    \n",
    "![Training Loss](./assets/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will compute the accuracy of the neural network. You have to complete the steps outlined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeScore(X, y):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_size = 1\n",
    "    my_nn.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, X.shape[0], batch_size):\n",
    "            # TO COMPLETE:\n",
    "            # - get the `batch_size` number of input samples\n",
    "            # - compute the prediction of the neural network\n",
    "            # - get the max of the prediction (e.g. get the most likely class)\n",
    "            # This can be done using `torch.max`.\n",
    "            # - get the max of the target (e.g. correct class)\n",
    "            # - check if the prediction is correct and count it\n",
    "            # - count every sample\n",
    "            \n",
    "    accuracy = correct/total * 100\n",
    "    print(f\"Accuracy of the network on the {total} samples: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeScore(X_train, y_train)\n",
    "computeScore(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute the score, and have done everything correctly (and you are lucky), you should have an accuracy of 90% and above.\n",
    "```\n",
    "Accuracy of the network on the 120 samples: 96.67%\n",
    "Accuracy of the network on the 30 samples: 96.67%\n",
    "```\n",
    "\n",
    "However, you might notice, if you repeat the runs, you might get totally different scores, even if all else stays the same.\n",
    "\n",
    "**Why does that happen ? Can you explain ?** *Hint: It might have something to do with degrees of freedom*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Explain here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional reading material\n",
    "[How to build a simple neural network in pytorch](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "\n",
    "[Introduction to optimizers](https://algorithmia.com/blog/introduction-to-optimizers)\n",
    "\n",
    "[What loss function to choose](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "\n",
    "[Visualizing optimizers (Interactive)](https://emiliendupont.github.io/2018/01/24/optimization-visualization/)\n",
    "\n",
    "[What are degrees of freedom?](https://machinelearningmastery.com/degrees-of-freedom-in-machine-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
